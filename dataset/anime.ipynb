{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d7d330",
   "metadata": {},
   "source": [
    "Remove irrelevant columns, apply logarithmic transformation to columns with long-tail distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e464e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取文件：/mnt/hdd/jiazy/anime/anime-dataset-2023.csv ...\n",
      "准备下载图片到: /mnt/hdd/jiazy/anime/images\n",
      "开始检查/下载图片 (下载失败的样本将被删除)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 24905/24905 [7:19:00<00:00,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "图片处理完成！剩余样本数: 24775 (已删除 130 条失败样本)\n",
      "正在删除 Name, Image URL 等无关列...\n",
      "正在处理 Genres 列 (只保留第一个题材)...\n",
      "Genres 处理示例: ['Action', 'Action', 'Action']\n",
      "正在对高方差列 ['Favorites', 'Popularity', 'Members'] 进行对数变换(Log1p)...\n",
      "正在对分类列进行编码 (Label Encoding)...\n",
      " - Type (前3个类别): [('Movie', 0), ('Music', 1), ('ONA', 2)]\n",
      " - Status (前3个类别): [('Currently Airing', 0), ('Finished Airing', 1), ('Not yet aired', 2)]\n",
      " - Source (前3个类别): [('4-koma manga', 0), ('Book', 1), ('Card game', 2)]\n",
      " - Rating (前3个类别): [('G - All Ages', 0), ('PG - Children', 1), ('PG-13 - Teens 13 or older', 2)]\n",
      " - Genres (前3个类别): [('Action', 0), ('Adventure', 1), ('Avant Garde', 2)]\n",
      "\n",
      "处理完成！前5行数据预览：\n",
      "   anime_id Score  Genres  Type Episodes  Status  Source       Duration  \\\n",
      "0         1  8.75       0     5     26.0       1       9  24 min per ep   \n",
      "1         5  8.38       0     0      1.0       1       9    1 hr 55 min   \n",
      "2         6  8.22       0     5     26.0       1       5  24 min per ep   \n",
      "3         7  7.25       0     5     26.0       1       9  25 min per ep   \n",
      "4         8  6.94       1     5     52.0       1       5  23 min per ep   \n",
      "\n",
      "   Rating  Popularity  Favorites Scored By    Members  \n",
      "0       3    3.784190  11.271185  914193.0  14.387341  \n",
      "1       3    6.401917   7.278629  206248.0  12.796575  \n",
      "2       2    5.509388   9.618203  356739.0  13.497030  \n",
      "3       2    7.493317   6.419995   42829.0  11.625647  \n",
      "4       1    8.542276   2.708050    6413.0   9.615939  \n",
      "\n",
      "已保存预处理后的 CSV 到: /mnt/hdd/jiazy/anime/anime_preprocessed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= Configure paths =================\n",
    "BASE_DIR = '/data1/jiazy/anime'\n",
    "FILE_PATH = os.path.join(BASE_DIR, 'anime-dataset-2023.csv')\n",
    "IMAGE_SAVE_DIR = os.path.join(BASE_DIR, 'images')  # Image save directory\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, 'anime_preprocessed.csv')\n",
    "# ===========================================\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"Download a single image, return True on success, False on failure\"\"\"\n",
    "    try:\n",
    "        if pd.isna(url) or url == 'Unknown':\n",
    "            return False\n",
    "        # Set timeout\n",
    "        response = requests.get(url, timeout=(5, 10))\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# 1. Load data\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    print(f\"Error: File not found {FILE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Reading file: {FILE_PATH} ...\")\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "original_count = len(df)\n",
    "\n",
    "# ================= Step A: Download images and filter failed samples =================\n",
    "print(f\"Preparing to download images to: {IMAGE_SAVE_DIR}\")\n",
    "if not os.path.exists(IMAGE_SAVE_DIR):\n",
    "    os.makedirs(IMAGE_SAVE_DIR)\n",
    "\n",
    "valid_indices = []\n",
    "\n",
    "if 'anime_id' in df.columns and 'Image URL' in df.columns:\n",
    "    print(\"Starting to check/download images (samples with failed downloads will be removed)...\")\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Images\"):\n",
    "        img_url = row['Image URL']\n",
    "        anime_id = row['anime_id']\n",
    "        save_path = os.path.join(IMAGE_SAVE_DIR, f\"{anime_id}.jpg\")\n",
    "        \n",
    "        is_valid = False\n",
    "        if os.path.exists(save_path):\n",
    "            is_valid = True\n",
    "        else:\n",
    "            is_valid = download_image(img_url, save_path)\n",
    "            \n",
    "        if is_valid:\n",
    "            valid_indices.append(index)\n",
    "            \n",
    "    # Filter valid samples\n",
    "    df = df.loc[valid_indices].reset_index(drop=True)\n",
    "    print(f\"\\nImage processing completed! Remaining samples: {len(df)} (removed {original_count - len(df)} failed samples)\")\n",
    "else:\n",
    "    print(\"Warning: 'anime_id' or 'Image URL' column not found, skipping download.\")\n",
    "\n",
    "# ================= Step B: Remove irrelevant columns =================\n",
    "# Note: Genres has been removed from here\n",
    "cols_to_drop = [\n",
    "    'Name', 'English name', 'Other name', 'Synopsis', \n",
    "    'Aired', 'Premiered', 'Producers', 'Licensors', 'Studios', 'Rank',\n",
    "    'Image URL' \n",
    "]\n",
    "\n",
    "print(\"Removing irrelevant columns like Name, Image URL...\")\n",
    "df.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "# ================= Step C: Process Genres (keep only the first) =================\n",
    "if 'Genres' in df.columns:\n",
    "    print(\"Processing Genres column (keep only the first genre)...\")\n",
    "    # 1. Fill null values\n",
    "    df['Genres'] = df['Genres'].fillna('Unknown').astype(str)\n",
    "    # 2. Split string and take the first, remove leading/trailing spaces\n",
    "    df['Genres'] = df['Genres'].apply(lambda x: x.split(',')[0].strip())\n",
    "    \n",
    "    print(f\"Genres processing example: {df['Genres'].head(3).tolist()}\")\n",
    "\n",
    "# ================= Step D: Logarithmic transformation =================\n",
    "cols_to_log = ['Favorites', 'Popularity', 'Members'] \n",
    "print(f\"Applying logarithmic transformation (Log1p) to high-variance columns {cols_to_log}...\")\n",
    "\n",
    "for col in cols_to_log:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "# ================= Step E: Categorical feature encoding (including Genres) =================\n",
    "# Add Genres to encoding list\n",
    "cols_to_encode = ['Type', 'Status', 'Source', 'Rating', 'Genres']\n",
    "le = LabelEncoder()\n",
    "\n",
    "print(\"Encoding categorical columns (Label Encoding)...\")\n",
    "for col in cols_to_encode:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Unknown').astype(str)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "        # Print encoding mapping example\n",
    "        mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        print(f\" - {col} (first 3 categories): {list(mapping.items())[:3]}\")\n",
    "\n",
    "# ================= Step F: Save results =================\n",
    "print(\"\\nProcessing completed! Preview of first 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nSaved preprocessed CSV to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cff84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/jiazy/miniconda3/envs/charms/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始执行 Anime 数据集预处理流程 ===\n",
      "正在加载数据 /data1/jiazy/anime/anime_preprocessed.csv...\n",
      "初始记录数: 24775\n",
      "正在清洗数据...\n",
      "正在清洗标签列 Score (强制转数字，删除无效标签)...\n",
      " - 因标签缺失删除了 9133 条样本\n",
      "正在清洗连续特征 (将 UNKNOWN 填充为众数): ['Members', 'Favorites', 'Popularity', 'Episodes', 'Duration', 'Scored By'] ...\n",
      "正在处理类别特征...\n",
      " - Genres: 22 类\n",
      " - Type: 7 类\n",
      " - Status: 2 类\n",
      " - Source: 17 类\n",
      " - Rating: 7 类\n",
      "正在进行 80:10:10 随机划分...\n",
      "划分结果: Train=12513, Val=1564, Test=1565\n",
      "正在标准化连续特征: ['Members', 'Favorites', 'Popularity', 'Episodes', 'Duration', 'Scored By'] ...\n",
      "正在处理图像 (Resize 224x224 -> .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train: 100%|██████████| 12513/12513 [00:00<00:00, 346088.78it/s]\n",
      "Processing Val: 100%|██████████| 1564/1564 [00:00<00:00, 330539.73it/s]\n",
      "Processing Test: 100%|██████████| 1565/1565 [00:00<00:00, 330384.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在保存最终特征到 ./features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Anime 预处理全部完成！ ===\n",
      "输出目录: /data1/jiazy/anime/features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 确保 PIL 版本兼容\n",
    "try:\n",
    "    LANCZOS_RESAMPLE = Image.Resampling.LANCZOS\n",
    "except AttributeError:\n",
    "    LANCZOS_RESAMPLE = Image.LANCZOS\n",
    "\n",
    "# ================= 1. Configure paths and constants (modify according to your environment) =================\n",
    "# Root directory\n",
    "BASE_DIR = \"/data1/jiazy/anime\"\n",
    "\n",
    "# Input file (use the preprocessed CSV generated in the previous step)\n",
    "METADATA_FILE = os.path.join(BASE_DIR, \"anime_preprocessed.csv\")\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images\")\n",
    "\n",
    "# Output directory (feature save location)\n",
    "OUTPUT_DIR = \"./features\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# ================= 2. Define feature columns =================\n",
    "# Continuous features (Log transformation already done previously, will perform StandardScale here)\n",
    "CONTINUOUS_COLS = [\n",
    "    'Members', 'Favorites', 'Popularity','Episodes', 'Duration', 'Scored By'\n",
    "]\n",
    "\n",
    "# Categorical features (LabelEncoder already done previously, will reorganize here to ensure compatibility)\n",
    "CATEGORICAL_COLS = [\n",
    "    'Genres', 'Type', 'Status', 'Source', 'Rating'\n",
    "]\n",
    "\n",
    "# Label column (regression target)\n",
    "LABEL_COL = 'Score'\n",
    "\n",
    "# ID column (for associating images)\n",
    "ID_COL = 'anime_id'\n",
    "\n",
    "# Non-feature columns to exclude (if any)\n",
    "NON_FEATURE_COLS = []\n",
    "\n",
    "# ================= 3. Image processing helper function =================\n",
    "def process_and_save_image(img_path_info):\n",
    "    \"\"\"\n",
    "    Receive tuple (original jpg path, target npy path)\n",
    "    Load JPG -> Resize 224x224 -> Save .npy\n",
    "    \"\"\"\n",
    "    jpg_path, npy_path = img_path_info\n",
    "    \n",
    "    # If .npy already exists, return path directly (skip processing)\n",
    "    if os.path.exists(npy_path):\n",
    "        return npy_path\n",
    "\n",
    "    if not os.path.exists(jpg_path):\n",
    "        return None  # Original image missing\n",
    "\n",
    "    try:\n",
    "        # Open image\n",
    "        img = Image.open(jpg_path)\n",
    "        \n",
    "        # Force convert to RGB (prevent PNG transparency channel or grayscale errors)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # Resize to 224x224\n",
    "        img_resized = img.resize((224, 224), resample=LANCZOS_RESAMPLE)\n",
    "        \n",
    "        # Convert to NumPy array\n",
    "        np_img = np.array(img_resized)\n",
    "        \n",
    "        # Save as .npy\n",
    "        np.save(npy_path, np_img)\n",
    "        \n",
    "        return npy_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image {jpg_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_anime_flow():\n",
    "    print(\"=== Starting Anime dataset preprocessing pipeline ===\")\n",
    "    \n",
    "    # --- 1. Load data ---\n",
    "    if not os.path.exists(METADATA_FILE):\n",
    "        raise FileNotFoundError(f\"Input file not found: {METADATA_FILE}\")\n",
    "        \n",
    "    print(f\"Loading data {METADATA_FILE}...\")\n",
    "    df = pd.read_csv(METADATA_FILE)\n",
    "    print(f\"Initial record count: {len(df)}\")\n",
    "\n",
    "    # Construct image paths\n",
    "    df['image_path'] = df[ID_COL].apply(lambda x: os.path.join(IMAGE_DIR, f\"{x}.jpg\"))\n",
    "    df['npy_path_target'] = df['image_path'].apply(lambda x: x.replace(\".jpg\", \".npy\"))\n",
    "\n",
    "    # --- 2. Data cleaning (key modification part) ---\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    # === A. Clean label column (LABEL_COL) ===\n",
    "    # Must execute this step first, convert 'UNKNOWN' to NaN, then dropna\n",
    "    print(f\"Cleaning label column {LABEL_COL} (force convert to numeric, remove invalid labels)...\")\n",
    "    \n",
    "    # 1. Force convert to numeric, 'UNKNOWN' becomes NaN\n",
    "    df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors='coerce')\n",
    "    \n",
    "    # 2. Delete rows with NaN labels (data without Ground Truth cannot be used for training)\n",
    "    original_len = len(df)\n",
    "    df = df.dropna(subset=[LABEL_COL])\n",
    "    print(f\" - Removed {original_len - len(df)} samples due to missing labels\")\n",
    "    \n",
    "    # 3. Ensure label is float type\n",
    "    df[LABEL_COL] = df[LABEL_COL].astype(float)\n",
    "    # =================================\n",
    "    \n",
    "    # === B. Clean continuous feature columns (CONTINUOUS_COLS) ===\n",
    "    print(f\"Cleaning continuous features (fill UNKNOWN with mode): {CONTINUOUS_COLS} ...\")\n",
    "    for col in CONTINUOUS_COLS:\n",
    "        # 1. Force convert to numeric\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        # 2. Calculate mode\n",
    "        if not df[col].mode().empty:\n",
    "            mode_val = df[col].mode()[0]\n",
    "        else:\n",
    "            mode_val = 0\n",
    "        # 3. Fill NaN\n",
    "        df[col] = df[col].fillna(mode_val).astype(float)\n",
    "    # ===========================================\n",
    "\n",
    "    # --- 3. Process categorical features ---\n",
    "    print(\"Processing categorical features...\")\n",
    "    cat_dims = [] \n",
    "    for col in CATEGORICAL_COLS:\n",
    "        # Categorical features also need to guard against dirty data\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        df[col] = df[col].astype('category')\n",
    "        df[col] = df[col].cat.codes\n",
    "        \n",
    "        dim = int(df[col].max() + 1)\n",
    "        cat_dims.append(dim)\n",
    "        print(f\" - {col}: {dim} classes\")\n",
    "\n",
    "    # --- 4. Dataset splitting ---\n",
    "    print(\"Performing 80:10:10 random split...\")\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Split results: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "\n",
    "    # --- 5. Continuous feature standardization ---\n",
    "    if CONTINUOUS_COLS:\n",
    "        print(f\"Standardizing continuous features: {CONTINUOUS_COLS} ...\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[CONTINUOUS_COLS])\n",
    "        \n",
    "        train_df[CONTINUOUS_COLS] = scaler.transform(train_df[CONTINUOUS_COLS])\n",
    "        val_df[CONTINUOUS_COLS] = scaler.transform(val_df[CONTINUOUS_COLS])\n",
    "        test_df[CONTINUOUS_COLS] = scaler.transform(test_df[CONTINUOUS_COLS])\n",
    "\n",
    "    # --- 6. Image processing ---\n",
    "    print(\"Processing images (Resize 224x224 -> .npy)...\")\n",
    "    for split_name, split_df in zip(['Train', 'Val', 'Test'], [train_df, val_df, test_df]):\n",
    "        path_pairs = zip(split_df['image_path'], split_df['npy_path_target'])\n",
    "        results = []\n",
    "        for pair in tqdm(path_pairs, total=len(split_df), desc=f\"Processing {split_name}\"):\n",
    "            results.append(process_and_save_image(pair))\n",
    "        split_df['final_npy_path'] = results\n",
    "\n",
    "    # --- 7. Filter again ---\n",
    "    train_df = train_df.dropna(subset=['final_npy_path'])\n",
    "    val_df = val_df.dropna(subset=['final_npy_path'])\n",
    "    test_df = test_df.dropna(subset=['final_npy_path'])\n",
    "\n",
    "    # --- 8. Save output ---\n",
    "    print(f\"Saving final features to {OUTPUT_DIR} ...\")\n",
    "\n",
    "    # Save tabular_lengths\n",
    "    tabular_lengths = cat_dims + [1] * len(CONTINUOUS_COLS)\n",
    "    torch.save(tabular_lengths, os.path.join(OUTPUT_DIR, \"tabular_lengths.pt\"))\n",
    "\n",
    "    for split_name, df_split in zip([\"train\", \"val\", \"test\"], [train_df, val_df, test_df]):\n",
    "        # A. Features CSV\n",
    "        features_path = os.path.join(OUTPUT_DIR, f\"{split_name}_features.csv\")\n",
    "        cols_to_save = CATEGORICAL_COLS + CONTINUOUS_COLS\n",
    "        df_split[cols_to_save].to_csv(features_path, index=False, header=False)\n",
    "        \n",
    "        # B. Labels Tensor (this previously had errors, should be fixed now)\n",
    "        labels_path = os.path.join(OUTPUT_DIR, f\"{split_name}_labels.pt\")\n",
    "        # At this point df_split[LABEL_COL] is already float type, won't error anymore\n",
    "        labels_tensor = torch.tensor(df_split[LABEL_COL].values, dtype=torch.float32)\n",
    "        torch.save(labels_tensor, labels_path)\n",
    "        \n",
    "        # C. Paths List\n",
    "        paths_path = os.path.join(OUTPUT_DIR, f\"{split_name}_paths.pt\")\n",
    "        npy_path_list = df_split['final_npy_path'].tolist()\n",
    "        torch.save(npy_path_list, paths_path)\n",
    "\n",
    "    print(\"\\n=== Anime preprocessing completed! ===\")\n",
    "    print(f\"Output directory: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_anime_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
