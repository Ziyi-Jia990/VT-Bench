{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Path configuration\n",
    "BASE_DIR = \"/data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/csv\"\n",
    "\n",
    "dicom_path = os.path.join(BASE_DIR, \"dicom_info.csv\")\n",
    "calc_train_path = os.path.join(BASE_DIR, \"calc_case_description_train_set.csv\")\n",
    "mass_train_path = os.path.join(BASE_DIR, \"mass_case_description_train_set.csv\")\n",
    "calc_test_path = os.path.join(BASE_DIR, \"calc_case_description_test_set.csv\")\n",
    "mass_test_path = os.path.join(BASE_DIR, \"mass_case_description_test_set.csv\")\n",
    "\n",
    "OUTPUT_TRAIN_PATH = \"./train_cleaned.csv\"\n",
    "OUTPUT_TEST_PATH = \"./test_cleaned.csv\"\n",
    "\n",
    "def extract_uids(path):\n",
    "    \"\"\"Extract StudyInstanceUID and SeriesInstanceUID from the file path using regex.\"\"\"\n",
    "    uids = re.findall(r\"1\\.3\\.6\\.1\\.4\\.1\\.9590\\.[\\d\\.]+\", str(path))\n",
    "    if len(uids) >= 2:\n",
    "        return pd.Series({\"StudyInstanceUID\": uids[0], \"SeriesInstanceUID\": uids[1]})\n",
    "    else:\n",
    "        return pd.Series({\"StudyInstanceUID\": None, \"SeriesInstanceUID\": None})\n",
    "\n",
    "def normalize_pathology(x):\n",
    "    \"\"\"Normalize pathology labels into standard categories.\"\"\"\n",
    "    x = str(x).strip().upper()\n",
    "    if \"MALIGNANT\" in x:\n",
    "        return \"MALIGNANT\"\n",
    "    elif \"BENIGN_WITHOUT_CALLBACK\" in x:\n",
    "        return \"BENIGN_WITHOUT_CALLBACK\"\n",
    "    elif \"BENIGN\" in x:\n",
    "        return \"BENIGN\"\n",
    "    return x\n",
    "\n",
    "def merge_with_dicom(df, dicom, lesion_type):\n",
    "    \"\"\"Merge lesion metadata with DICOM info to retrieve final image paths.\"\"\"\n",
    "    df[\"lesion_type\"] = lesion_type\n",
    "\n",
    "    # Extract UIDs from the provided path column\n",
    "    uid_info = df[\"image file path\"].apply(extract_uids)\n",
    "    df = pd.concat([df, uid_info], axis=1)\n",
    "\n",
    "    df[\"pathology\"] = df[\"pathology\"].apply(normalize_pathology)\n",
    "\n",
    "    # Filter for full mammogram images only\n",
    "    dicom_full = dicom[dicom[\"SeriesDescription\"].str.contains(\"full\", case=False, na=False)]\n",
    "    dicom_unique = dicom_full.drop_duplicates(subset=[\"StudyInstanceUID\", \"SeriesInstanceUID\"], keep=\"first\")\n",
    "\n",
    "    merged = df.merge(\n",
    "        dicom_unique,\n",
    "        on=[\"StudyInstanceUID\", \"SeriesInstanceUID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    before = len(merged)\n",
    "    # Handle potential column name conflicts after merge\n",
    "    image_path_col = 'image_path_y' if 'image_path_y' in merged.columns else 'image_path'\n",
    "    \n",
    "    merged = merged.rename(columns={image_path_col: 'image_path_final'})\n",
    "    merged = merged.dropna(subset=['image_path_final']).reset_index(drop=True)\n",
    "    \n",
    "    after = len(merged)\n",
    "    print(f\"Match Results ({lesion_type}): {after}/{before} records ({after/before:.1%})\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def clean_calc_df(df):\n",
    "    \"\"\"Rename columns and handle missing values for calcification datasets.\"\"\"\n",
    "    df = df.rename(columns={\n",
    "        'calc type':'calc_type',\n",
    "        'calc distribution':'calc_distribution',\n",
    "        'image view':'image_view',\n",
    "        'left or right breast':'left_or_right_breast',\n",
    "        'breast density':'breast_density',\n",
    "        'abnormality type':'abnormality_type'\n",
    "    })\n",
    "    \n",
    "    if 'calc_type' in df.columns:\n",
    "        df['calc_type'] = df['calc_type'].bfill()\n",
    "    if 'calc_distribution' in df.columns:\n",
    "        df['calc_distribution'] = df['calc_distribution'].bfill()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_mass_df(df):\n",
    "    \"\"\"Rename columns and handle missing values for mass datasets.\"\"\"\n",
    "    df = df.rename(columns={\n",
    "        'mass shape':'mass_shape',\n",
    "        'mass margins':'mass_margins',\n",
    "        'image view':'image_view',\n",
    "        'left or right breast':'left_or_right_breast',\n",
    "        'abnormality type':'abnormality_type'\n",
    "    })\n",
    "    \n",
    "    if 'mass_shape' in df.columns:\n",
    "        df['mass_shape'] = df['mass_shape'].bfill()\n",
    "    if 'mass_margins' in df.columns:\n",
    "        df['mass_margins'] = df['mass_margins'].bfill()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def post_merge_clean(df):\n",
    "    \"\"\"Remove redundant columns and sensitive/irrelevant metadata after merging.\"\"\"\n",
    "    if 'SeriesDescription' in df.columns:\n",
    "        df['SeriesDescription'] = df['SeriesDescription'].bfill()\n",
    "    if 'Laterality' in df.columns:\n",
    "        df['Laterality'] = df['Laterality'].bfill()\n",
    "\n",
    "    COLS_TO_DROP = [\n",
    "        'PatientBirthDate','AccessionNumber','Columns','ContentDate','ContentTime',\n",
    "        'PatientSex','ReferringPhysicianName','Rows','SOPClassUID',\n",
    "        'SOPInstanceUID','StudyDate','StudyID','StudyInstanceUID','StudyTime',\n",
    "        'InstanceNumber','SeriesInstanceUID','SeriesNumber',\n",
    "        'image file path', 'cropped image file path', 'ROI mask file path',\n",
    "        'image_path_x', 'image_path', 'calc_type', 'calc_distribution', \n",
    "        'mass_shape', 'mass_margins', 'patient_id', 'lesion_type', \n",
    "        'PatientID', 'PatientName', 'abnormality_type', 'pathology', \n",
    "        'file_path', 'Laterality', 'PatientOrientation',\n",
    "    ]\n",
    "    \n",
    "    cols_to_remove = [col for col in COLS_TO_DROP if col in df.columns]\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execution Flow\n",
    "print(\"Loading datasets...\")\n",
    "dicom = pd.read_csv(dicom_path)\n",
    "calc_train = pd.read_csv(calc_train_path)\n",
    "mass_train = pd.read_csv(mass_train_path)\n",
    "calc_test = pd.read_csv(calc_test_path)\n",
    "mass_test = pd.read_csv(mass_test_path)\n",
    "\n",
    "print(\"Pre-cleaning description files...\")\n",
    "calc_train_cleaned = clean_calc_df(calc_train)\n",
    "mass_train_cleaned = clean_mass_df(mass_train)\n",
    "calc_test_cleaned = clean_calc_df(calc_test)\n",
    "mass_test_cleaned = clean_mass_df(mass_test)\n",
    "\n",
    "print(\"Merging with DICOM metadata...\")\n",
    "merged_calc_train = merge_with_dicom(calc_train_cleaned, dicom, \"calcification\")\n",
    "merged_mass_train = merge_with_dicom(mass_train_cleaned, dicom, \"mass\")\n",
    "merged_calc_test = merge_with_dicom(calc_test_cleaned, dicom, \"calcification\")\n",
    "merged_mass_test = merge_with_dicom(mass_test_cleaned, dicom, \"mass\")\n",
    "\n",
    "# Map 4-category labels\n",
    "label_map = {\n",
    "    (\"calcification\", \"MALIGNANT\"): 0,\n",
    "    (\"calcification\", \"BENIGN\"): 1,\n",
    "    (\"calcification\", \"BENIGN_WITHOUT_CALLBACK\"): 1,\n",
    "    (\"mass\", \"MALIGNANT\"): 2,\n",
    "    (\"mass\", \"BENIGN\"): 3,\n",
    "    (\"mass\", \"BENIGN_WITHOUT_CALLBACK\"): 3,\n",
    "}\n",
    "\n",
    "for df in [merged_calc_train, merged_mass_train, merged_calc_test, merged_mass_test]:\n",
    "    df[\"label_4c\"] = df.apply(\n",
    "        lambda r: label_map.get((r[\"lesion_type\"], r[\"pathology\"]), None), axis=1\n",
    "    )\n",
    "\n",
    "print(\"Combining datasets...\")\n",
    "train_df = pd.concat([merged_calc_train, merged_mass_train], ignore_index=True)\n",
    "test_df = pd.concat([merged_calc_test, merged_mass_test], ignore_index=True)\n",
    "\n",
    "# Shuffle training set\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Performing post-merge cleanup...\")\n",
    "train_df_final = post_merge_clean(train_df)\n",
    "test_df_final = post_merge_clean(test_df)\n",
    "\n",
    "# Final path string adjustment\n",
    "train_df_final['image_path_final'] = train_df_final['image_path_final'].astype(str).str.replace(\n",
    "    \"CBIS-DDSM\", \"cbis-ddsm-breast-cancer-image-dataset\"\n",
    ")\n",
    "test_df_final['image_path_final'] = test_df_final['image_path_final'].astype(str).str.replace(\n",
    "    \"CBIS-DDSM\", \"cbis-ddsm-breast-cancer-image-dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Saving results to {OUTPUT_TRAIN_PATH} and {OUTPUT_TEST_PATH}\")\n",
    "train_df_final.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "test_df_final.to_csv(OUTPUT_TEST_PATH, index=False)\n",
    "\n",
    "print(\"\\nLabel Distribution (label_4c):\")\n",
    "print(train_df_final[\"label_4c\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29785b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/jiazy/miniconda3/envs/charms/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2058650/1216999811.py:222: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534 -1.17317184 ...  0.30490534 -2.16027213\n",
      "  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_split.loc[:, final_continuous_cols] = scaler.transform(train_split[final_continuous_cols])\n",
      "/tmp/ipykernel_2058650/1216999811.py:223: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.54214304  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534 -2.37539618\n",
      "  0.30490534  0.30490534 -3.46336803  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.28481763 -2.74954733\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.8319249   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      " -2.62500183  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.55706792  0.30490534  0.30490534  0.30490534 -2.97599369  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -4.29041191  0.30490534  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.44024218  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.9564704   0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.39495291  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.2426497   0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.09693664  0.30490534 -2.16850654\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.28221094  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -7.51984587  0.30490534 -1.89985881\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.35275154  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.7948366   0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.39804081  0.30490534\n",
      "  0.30490534  0.30490534 -1.87412627  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31621133  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534 -1.91118113  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.85144819  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -3.65584744  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -6.07573564  0.30490534 -2.61367951  0.30490534\n",
      "  0.30490534 -3.76546807  0.30490534  0.30490534 -2.27349531  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534\n",
      " -2.9193821   0.30490534 -1.79795794  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30439069 -2.95334906  0.30490534  0.30490534 -2.96621533  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.97911504  0.30490534 -1.74752216  0.30490534  0.30490534  0.30490534\n",
      " -1.19427252  0.30490534  0.30490534  0.30490534 -1.62760852  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.95334906  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.80615892  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.51177865  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.13450615  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.90133588  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -4.07528787 -4.7510244   0.30490534 -2.34142922\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.10366054  0.30439069  0.30490534  0.30490534 -2.13762749\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -3.3841118   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31878458  0.30490534  0.30490534 -3.51997962  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -0.89834829  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  val_split.loc[:, final_continuous_cols]   = scaler.transform(val_split[final_continuous_cols])\n",
      "/tmp/ipykernel_2058650/1216999811.py:268: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534 -1.17317184 ...  0.30490534 -2.16027213\n",
      "  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_split_tip.loc[:, final_continuous_cols] = scaler.transform(train_split_tip[final_continuous_cols])\n",
      "/tmp/ipykernel_2058650/1216999811.py:269: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.54214304  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534 -2.37539618\n",
      "  0.30490534  0.30490534 -3.46336803  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.28481763 -2.74954733\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.8319249   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      " -2.62500183  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.55706792  0.30490534  0.30490534  0.30490534 -2.97599369  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -4.29041191  0.30490534  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.44024218  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.9564704   0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.39495291  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.2426497   0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.09693664  0.30490534 -2.16850654\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.28221094  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -7.51984587  0.30490534 -1.89985881\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.35275154  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.7948366   0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.39804081  0.30490534\n",
      "  0.30490534  0.30490534 -1.87412627  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31621133  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534 -1.91118113  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.85144819  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -3.65584744  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -6.07573564  0.30490534 -2.61367951  0.30490534\n",
      "  0.30490534 -3.76546807  0.30490534  0.30490534 -2.27349531  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534\n",
      " -2.9193821   0.30490534 -1.79795794  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30439069 -2.95334906  0.30490534  0.30490534 -2.96621533  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.97911504  0.30490534 -1.74752216  0.30490534  0.30490534  0.30490534\n",
      " -1.19427252  0.30490534  0.30490534  0.30490534 -1.62760852  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.95334906  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.80615892  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.51177865  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.13450615  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.90133588  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -4.07528787 -4.7510244   0.30490534 -2.34142922\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.10366054  0.30439069  0.30490534  0.30490534 -2.13762749\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -3.3841118   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31878458  0.30490534  0.30490534 -3.51997962  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -0.89834829  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  val_split_tip.loc[:, final_continuous_cols]   = scaler.transform(val_split_tip[final_continuous_cols])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æº (CSV): /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/\n",
      "è¾“å‡ºç›®å½• (Processed): /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features\n",
      "\n",
      "Step 1/11: åŠ è½½å·²æ¸…ç†çš„ train_cleaned.csv å’Œ test_cleaned.csv...\n",
      "    Train DF (loaded): 2864 è¡Œ, Test DF (loaded): 422 è¡Œ\n",
      "Step 2/11: å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)...\n",
      "    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ 7 ä¸ªè¿ç»­ç‰¹å¾: ['BitsAllocated', 'BitsStored', 'HighBit', 'LargestImagePixelValue', 'PixelRepresentation', 'SamplesPerPixel', 'SmallestImagePixelValue']\n",
      "    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ 13 ä¸ªåˆ†ç±»ç‰¹å¾: ['left_or_right_breast', 'image_view', 'BodyPartExamined', 'ConversionType', 'Modality', 'PhotometricInterpretation', 'SecondaryCaptureDeviceManufacturer', 'SecondaryCaptureDeviceManufacturerModelName', 'SeriesDescription', 'SpecificCharacterSet', 'breast_density', 'assessment', 'subtlety']\n",
      "Step 3/11: æ£€æŸ¥å¹¶åˆ é™¤ 'æ’å®š' ç‰¹å¾...\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BitsAllocated\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BitsStored\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): HighBit\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): PixelRepresentation\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SamplesPerPixel\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SmallestImagePixelValue\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BodyPartExamined\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): ConversionType\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): Modality\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): PhotometricInterpretation\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SecondaryCaptureDeviceManufacturer\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SecondaryCaptureDeviceManufacturerModelName\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SeriesDescription\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SpecificCharacterSet\n",
      "    âœ… å·²ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤ 14 ä¸ªæ’å®šåˆ—ã€‚\n",
      "    è·¯å¾„è½¬æ¢å®Œæˆã€‚ç¤ºä¾‹: /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/jpeg/1.3.6.1.4.1.9590.100.1.2.148086015311392424203944628703986309919/1-064.jpg\n",
      "Step 4/11: è¿‡æ»¤æ— æ•ˆçš„æ ‡ç­¾/è·¯å¾„...\n",
      "    è¿‡æ»¤åå‰©ä½™: Train 2864, Test 422\n",
      "Step 5/11: å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)...\n",
      "-> æ­£åœ¨ä¸º _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬åˆ›å»ºæ•°æ®å¤‡ä»½...\n",
      "Step 6/11: ç¼–ç åˆ†ç±»ç‰¹å¾...\n",
      "Step 6.B/11: ç¼–ç åˆ†ç±»ç‰¹å¾ [_TIP ç‰ˆ]...\n",
      "Step 7/11: åˆ’åˆ† Train/Val (80/20)ï¼ŒåŸºäº label_4c åˆ†å±‚...\n",
      "    Train: 2291, Val: 573, Test: 422\n",
      "Step 8/11: æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\n",
      "Step 9/11: ä¿å­˜å¤„ç†åçš„ç‰¹å¾ CSV (æ— è¡¨å¤´) å’Œ field_lengths.pt...\n",
      "ğŸ’¾ Saved train_features.csv\n",
      "ğŸ’¾ Saved val_features.csv\n",
      "ğŸ’¾ Saved test_features.csv\n",
      "ğŸ’¾ Saved tabular_lengths.pt (Features: 6, Lengths: [1, 2, 2, 4, 6, 6])\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å¹¶ä¿å­˜ _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬ ---\n",
      "    Step 7_TIP: åˆ’åˆ† _TIP æ•°æ® (ä½¿ç”¨ç›¸åŒç´¢å¼•)...\n",
      "    Step 8_TIP: æ ‡å‡†åŒ– _TIP æ•°æ® (é‡ç”¨ scaler)...\n",
      "    Step 9_TIP: ä¿å­˜ _TIP ç‰¹å¾ CSV...\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/train_features_TIP.csv\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/val_features_TIP.csv\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/test_features_TIP.csv\n",
      "--- _TIP ç‰ˆæœ¬å¤„ç†å®Œæ¯• ---\n",
      "\n",
      "Step 10/11: ä¿å­˜ 4-class æ ‡ç­¾ .pt...\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/train_labels.pt (Classes: [0 1 2 3])\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/val_labels.pt (Classes: [0 1 2 3])\n",
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/test_labels.pt (Classes: [0 1 2 3])\n",
      "Step 11/11: è½¬æ¢å›¾åƒä¸º 224x224 .npy å¹¶è¿›è¡Œå½’ä¸€åŒ–...\n",
      "\n",
      "æ­£åœ¨å¤„ç† train å›¾åƒ: Resize(224x224) + RGBè½¬æ¢ + å½’ä¸€åŒ–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2291/2291 [10:19<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/train_paths.pt (åŒ…å« 2291 æ¡å¤„ç†åçš„ .npy è·¯å¾„)\n",
      "\n",
      "æ­£åœ¨å¤„ç† val å›¾åƒ: Resize(224x224) + RGBè½¬æ¢ + å½’ä¸€åŒ–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573/573 [02:32<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/val_paths.pt (åŒ…å« 573 æ¡å¤„ç†åçš„ .npy è·¯å¾„)\n",
      "\n",
      "æ­£åœ¨å¤„ç† test å›¾åƒ: Resize(224x224) + RGBè½¬æ¢ + å½’ä¸€åŒ–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422/422 [01:56<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/features/test_paths.pt (åŒ…å« 422 æ¡å¤„ç†åçš„ .npy è·¯å¾„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# è·¯å¾„é…ç½®\n",
    "# ==============================\n",
    "# 1. åŒ…å« train_cleaned.csv å’Œ test_cleaned.csv çš„ç›®å½•\n",
    "DATASET_ROOT = \"/data1/jiazy/tab_image_bench/cbis-ddsm-breast-cancer-image-dataset/\" \n",
    "BASE_IMAGE_PATH = \"/data1/jiazy/tab_image_bench/\"\n",
    "# 2. å¤„ç†åæ–‡ä»¶çš„è¾“å‡ºç›®å½•\n",
    "OUT_DIR = os.path.join(DATASET_ROOT, \"features\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 3. (å·²æ›´æ–°) å®šä¹‰æ‚¨çš„è¾“å…¥æ–‡ä»¶\n",
    "TRAIN_CLEANED_CSV = os.path.join(DATASET_ROOT, \"train_cleaned.csv\")\n",
    "TEST_CLEANED_CSV = os.path.join(DATASET_ROOT, \"test_cleaned.csv\")\n",
    "\n",
    "print(f\"æ•°æ®æº (CSV): {DATASET_ROOT}\")\n",
    "print(f\"è¾“å‡ºç›®å½• (Processed): {OUT_DIR}\\n\")\n",
    "\n",
    "# ==============================\n",
    "# 1ï¸âƒ£ åŠ è½½å·²æ¸…ç†çš„ CSV\n",
    "# ==============================\n",
    "print(\"Step 1/11: åŠ è½½å·²æ¸…ç†çš„ train_cleaned.csv å’Œ test_cleaned.csv...\")\n",
    "if not os.path.exists(TRAIN_CLEANED_CSV) or not os.path.exists(TEST_CLEANED_CSV):\n",
    "    print(f\"ğŸ”´ é”™è¯¯ï¼šæ‰¾ä¸åˆ° {TRAIN_CLEANED_CSV} æˆ– {TEST_CLEANED_CSV}\")\n",
    "    print(\"è¯·ç¡®ä¿ DATASET_ROOT å˜é‡è®¾ç½®æ­£ç¡®ã€‚\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CLEANED_CSV)\n",
    "    test_df  = pd.read_csv(TEST_CLEANED_CSV)\n",
    "except Exception as e:\n",
    "    print(f\"ğŸ”´ é”™è¯¯ï¼šè¯»å– CSV å¤±è´¥: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"    Train DF (loaded): {len(train_df)} è¡Œ, Test DF (loaded): {len(test_df)} è¡Œ\")\n",
    "\n",
    "# ==============================\n",
    "# 2ï¸âƒ£ (å·²ä¿®æ”¹) å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)\n",
    "# ==============================\n",
    "print(\"Step 2/11: å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)...\")\n",
    "new_label_col = \"label_4c\"\n",
    "image_path_col_final = \"image_path_final\" \n",
    "\n",
    "# 1. æ¥è‡ªæ‚¨è¦æ±‚çš„éç‰¹å¾åˆ—\n",
    "# (æˆ‘ä»¬åˆå¹¶äº†æ‚¨æä¾›çš„åˆ—å’Œè„šæœ¬ä¸­åŸæœ‰çš„å…¶ä»–éç‰¹å¾åˆ—)\n",
    "non_feature_cols_from_script = [\n",
    "    \"pathology\", \"abnormality type\", \"lesion_type\",\n",
    "    \"patient_id\", \"file_path\", \"abnormality id\"\n",
    "]\n",
    "non_feature_cols = list(set(\n",
    "    [new_label_col, image_path_col_final] + non_feature_cols_from_script\n",
    "))\n",
    "\n",
    "# 2. æ¥è‡ªæ‚¨è¦æ±‚çš„è¿ç»­ç‰¹å¾\n",
    "final_continuous_cols = [\n",
    "    'BitsAllocated', 'BitsStored', 'HighBit', 'LargestImagePixelValue', \n",
    "    'PixelRepresentation', 'SamplesPerPixel', 'SmallestImagePixelValue'\n",
    "]\n",
    "\n",
    "# 3. æ¥è‡ªæ‚¨è¦æ±‚çš„ç±»åˆ«ç‰¹å¾\n",
    "final_categorical_cols = [\n",
    "    'left_or_right_breast', 'image_view', 'BodyPartExamined', 'ConversionType', \n",
    "    'Modality', 'PhotometricInterpretation', 'SecondaryCaptureDeviceManufacturer', \n",
    "    'SecondaryCaptureDeviceManufacturerModelName', 'SeriesDescription', \n",
    "    'SpecificCharacterSet', 'breast_density', 'assessment', 'subtlety'\n",
    "]\n",
    "\n",
    "# ç¡®ä¿è¿™äº›åˆ—å­˜åœ¨äº DataFrame ä¸­ï¼Œè¿‡æ»¤æ‰ä¸å­˜åœ¨çš„åˆ—\n",
    "all_feature_cols_requested = final_continuous_cols + final_categorical_cols\n",
    "available_cols = set(train_df.columns)\n",
    "\n",
    "final_continuous_cols = [col for col in final_continuous_cols if col in available_cols]\n",
    "final_categorical_cols = [col for col in final_categorical_cols if col in available_cols]\n",
    "\n",
    "missing_cols = [col for col in all_feature_cols_requested if col not in available_cols]\n",
    "if missing_cols:\n",
    "    print(f\"    âš ï¸ è­¦å‘Š: æ‚¨æŒ‡å®šçš„ä¸€äº›ç‰¹å¾åˆ—åœ¨ train_df.csv ä¸­ä¸å­˜åœ¨ï¼Œå°†è¢«å¿½ç•¥: {missing_cols}\")\n",
    "\n",
    "print(f\"    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ {len(final_continuous_cols)} ä¸ªè¿ç»­ç‰¹å¾: {final_continuous_cols}\")\n",
    "print(f\"    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ {len(final_categorical_cols)} ä¸ªåˆ†ç±»ç‰¹å¾: {final_categorical_cols}\")\n",
    "\n",
    "# ==============================\n",
    "# 3ï¸âƒ£ (æ–°) åˆ é™¤åœ¨ train/test ä¸­å‡åªæœ‰ä¸€ä¸ªå”¯ä¸€å€¼çš„åˆ—\n",
    "# ==============================\n",
    "print(\"Step 3/11: æ£€æŸ¥å¹¶åˆ é™¤ 'æ’å®š' ç‰¹å¾...\")\n",
    "cols_to_drop = []\n",
    "all_feature_cols = final_continuous_cols + final_categorical_cols\n",
    "\n",
    "for col in all_feature_cols:\n",
    "    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼Œä»¥é˜²ä¸‡ä¸€\n",
    "    if col not in train_df.columns or col not in test_df.columns:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # nunique() é»˜è®¤ä¸è®¡ç®— NaN\n",
    "        train_unique_count = train_df[col].nunique()\n",
    "        test_unique_count = test_df[col].nunique()\n",
    "        \n",
    "        # å¦‚æœåœ¨ train å’Œ test ä¸­ï¼Œå”¯ä¸€å€¼çš„æ•°é‡éƒ½ä¸º 1 (æˆ– 0)\n",
    "        if train_unique_count <= 1 and test_unique_count <= 1:\n",
    "            cols_to_drop.append(col)\n",
    "            print(f\"    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: {train_unique_count}, Test å”¯ä¸€å€¼: {test_unique_count}): {col}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ æ£€æŸ¥ {col} å”¯ä¸€å€¼æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤è¿™äº›åˆ—\n",
    "final_continuous_cols = [col for col in final_continuous_cols if col not in cols_to_drop]\n",
    "final_categorical_cols = [col for col in final_categorical_cols if col not in cols_to_drop]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"    âœ… å·²ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤ {len(cols_to_drop)} ä¸ªæ’å®šåˆ—ã€‚\")\n",
    "else:\n",
    "    print(\"    âœ… æœªå‘ç°æ’å®šç‰¹å¾åˆ—ã€‚\")\n",
    "\n",
    "def make_abs_path(path):\n",
    "    # å¦‚æœè·¯å¾„æ˜¯ NaN æˆ– Noneï¼Œä¿æŒåŸæ ·\n",
    "    if pd.isna(path):\n",
    "        return path\n",
    "    # ä½¿ç”¨ os.path.join æ™ºèƒ½åœ°æ‹¼æ¥åŸºç¡€è·¯å¾„å’Œç›¸å¯¹è·¯å¾„\n",
    "    return os.path.join(BASE_IMAGE_PATH, str(path))\n",
    "\n",
    "train_df[image_path_col_final] = train_df[image_path_col_final].apply(make_abs_path)\n",
    "test_df[image_path_col_final] = test_df[image_path_col_final].apply(make_abs_path)\n",
    "\n",
    "print(f\"    è·¯å¾„è½¬æ¢å®Œæˆã€‚ç¤ºä¾‹: {train_df[image_path_col_final].iloc[0]}\")\n",
    "\n",
    "# ==============================\n",
    "# 4ï¸âƒ£ (åŸ Step 3) è¿‡æ»¤æ ‡ç­¾/è·¯å¾„\n",
    "# ==============================\n",
    "print(\"Step 4/11: è¿‡æ»¤æ— æ•ˆçš„æ ‡ç­¾/è·¯å¾„...\")\n",
    "# (æ‚¨çš„ info æ˜¾ç¤º 2864 non-nullï¼Œä½†è¿™ä¸€æ­¥æ˜¯ç¡®ä¿å®‰å…¨çš„)\n",
    "train_df.dropna(subset=[image_path_col_final, new_label_col], inplace=True)\n",
    "test_df.dropna(subset=[image_path_col_final, new_label_col], inplace=True)\n",
    "print(f\"    è¿‡æ»¤åå‰©ä½™: Train {len(train_df)}, Test {len(test_df)}\")\n",
    "\n",
    "# ==============================\n",
    "# 5ï¸âƒ£ (åŸ Step 4) å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)\n",
    "# ==============================\n",
    "print(\"Step 5/11: å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)...\")\n",
    "# (ä½¿ç”¨ .loc é¿å… FutureWarning)\n",
    "print(\"-> æ­£åœ¨ä¸º _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬åˆ›å»ºæ•°æ®å¤‡ä»½...\")\n",
    "train_df_tip = train_df.copy()\n",
    "test_df_tip = test_df.copy()\n",
    "\n",
    "\n",
    "for col in final_continuous_cols:\n",
    "    mean_val = train_df[col].mean()\n",
    "    train_df.loc[:, col] = train_df[col].fillna(mean_val)\n",
    "    test_df.loc[:, col]  = test_df[col].fillna(mean_val)\n",
    "\n",
    "for col in final_categorical_cols:\n",
    "    train_df.loc[:, col] = train_df[col].fillna(\"MISSING\")\n",
    "    test_df.loc[:, col]  = test_df[col].fillna(\"MISSING\")\n",
    "\n",
    "# ==============================\n",
    "# 6ï¸âƒ£ (åŸ Step 5) ç¼–ç åˆ†ç±»ç‰¹å¾\n",
    "# ==============================\n",
    "print(\"Step 6/11: ç¼–ç åˆ†ç±»ç‰¹å¾...\")\n",
    "field_lengths = [] \n",
    "# æ£€æŸ¥ï¼šå¦‚æœåˆ é™¤äº†æ‰€æœ‰åˆ†ç±»åˆ—ï¼Œåˆ™è·³è¿‡\n",
    "if final_categorical_cols:\n",
    "    full_df_imputed = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    for col in final_categorical_cols:\n",
    "        full_df_imputed[col] = full_df_imputed[col].astype('category')\n",
    "        codes = full_df_imputed[col].cat.codes\n",
    "        cardinality = len(full_df_imputed[col].cat.categories)\n",
    "        field_lengths.append(cardinality)\n",
    "        train_df[col] = codes.iloc[:len(train_df)].values\n",
    "        test_df[col]  = codes.iloc[len(train_df):].values\n",
    "else:\n",
    "    print(\"    (è·³è¿‡ï¼šæœªæ‰¾åˆ°åˆ†ç±»ç‰¹å¾)\")\n",
    "\n",
    "print(\"Step 6.B/11: ç¼–ç åˆ†ç±»ç‰¹å¾ [_TIP ç‰ˆ]...\")\n",
    "if final_categorical_cols:\n",
    "    full_df_tip = pd.concat([train_df_tip, test_df_tip], ignore_index=True)\n",
    "    for col in final_categorical_cols:\n",
    "        full_df_tip[col] = full_df_tip[col].astype('category')\n",
    "        # .cat.codes ä¼šè‡ªåŠ¨å°† NaN æ˜ å°„ä¸º -1\n",
    "        codes_tip = full_df_tip[col].cat.codes\n",
    "        # (æˆ‘ä»¬ä¸éœ€è¦ä¸º TIP ç‰ˆæœ¬ä¿å­˜ field_lengths)\n",
    "        train_df_tip[col] = codes_tip.iloc[:len(train_df_tip)].values\n",
    "        test_df_tip[col]  = codes_tip.iloc[len(train_df_tip):].values\n",
    "# --- [æ–°å¢ç»“æŸ] ---\n",
    "\n",
    "# ==============================\n",
    "# 7ï¸âƒ£ (åŸ Step 6) åˆ’åˆ† train / val\n",
    "# ==============================\n",
    "print(f\"Step 7/11: åˆ’åˆ† Train/Val (80/20)ï¼ŒåŸºäº {new_label_col} åˆ†å±‚...\")\n",
    "\n",
    "if len(train_df) == 0:\n",
    "    print(\"ğŸ”´ é”™è¯¯ï¼štrain_df åœ¨è¿‡æ»¤åä¸ºç©ºï¼æ— æ³•è¿›è¡Œ train_test_splitã€‚\")\n",
    "    print(\"    è¯·æ£€æŸ¥æ‚¨çš„ train_cleaned.csv æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ 'image_path_final' å’Œ 'label_4c'ã€‚\")\n",
    "    exit(1)\n",
    "\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, test_size=0.2, random_state=42,\n",
    "    stratify=train_df[new_label_col] \n",
    ")\n",
    "test_split = test_df # test_df ä¿æŒä¸º test_cleaned.csv çš„å…¨éƒ¨å†…å®¹\n",
    "print(f\"    Train: {len(train_split)}, Val: {len(val_split)}, Test: {len(test_split)}\")\n",
    "train_indices = train_split.index\n",
    "val_indices = val_split.index\n",
    "\n",
    "# ==============================\n",
    "# 8ï¸âƒ£ (åŸ Step 7) æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾\n",
    "# ==============================\n",
    "print(\"Step 8/11: æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\")\n",
    "scaler = StandardScaler()\n",
    "if final_continuous_cols: \n",
    "    scaler.fit(train_split[final_continuous_cols])\n",
    "    # (ä½¿ç”¨ .loc é¿å… FutureWarning)\n",
    "    train_split.loc[:, final_continuous_cols] = scaler.transform(train_split[final_continuous_cols])\n",
    "    val_split.loc[:, final_continuous_cols]   = scaler.transform(val_split[final_continuous_cols])\n",
    "    test_split.loc[:, final_continuous_cols]  = scaler.transform(test_split[final_continuous_cols])\n",
    "else:\n",
    "    print(\"    (è·³è¿‡ï¼šæœªæ‰¾åˆ°è¿ç»­ç‰¹å¾)\")\n",
    "\n",
    "# ==============================\n",
    "# 9ï¸âƒ£ (åŸ Step 8) ä¿å­˜å¤„ç†åçš„ç‰¹å¾ & å­—æ®µé•¿åº¦\n",
    "# ==============================\n",
    "print(\"Step 9/11: ä¿å­˜å¤„ç†åçš„ç‰¹å¾ CSV (æ— è¡¨å¤´) å’Œ field_lengths.pt...\")\n",
    "# DVM-Car è„šæœ¬é¡ºåº: è¿ç»­å‹, ç„¶å åˆ†ç±»å‹\n",
    "ordered_feature_cols = final_continuous_cols + final_categorical_cols\n",
    "# ä¸ºè¿ç»­ç‰¹å¾æ·»åŠ  '1' åˆ° field_lengths çš„å¼€å¤´\n",
    "# (field_lengths ä»…åŒ…å«åˆ†ç±»ç‰¹å¾çš„åŸºæ•°)\n",
    "final_field_lengths = [1] * len(final_continuous_cols) + field_lengths\n",
    "\n",
    "for df, name in zip([train_split, val_split, test_split], [\"train\", \"val\", \"test\"]):\n",
    "    # ä»…å½“ ordered_feature_cols ä¸ä¸ºç©ºæ—¶æ‰ä¿å­˜\n",
    "    if ordered_feature_cols:\n",
    "        features_only_df = df[ordered_feature_cols]\n",
    "        # å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ dvm_features_..._physical_jittered_50.csv\n",
    "        out_path = os.path.join(OUT_DIR, f\"{name}_features.csv\")\n",
    "        features_only_df.to_csv(out_path, index=False, header=False) \n",
    "        print(f\"ğŸ’¾ Saved {name}_features.csv\")\n",
    "    else:\n",
    "        print(f\"    (è·³è¿‡ {name}_features.csvï¼šæœªæ‰¾åˆ°ä»»ä½•ç‰¹å¾)\")\n",
    "\n",
    "# å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ tabular_lengths_all_views_physical.pt\n",
    "torch.save(final_field_lengths, os.path.join(OUT_DIR, \"tabular_lengths.pt\"))\n",
    "print(f\"ğŸ’¾ Saved tabular_lengths.pt (Features: {len(final_field_lengths)}, Lengths: {final_field_lengths})\")\n",
    "\n",
    "# ==============================\n",
    "# 9ï¸âƒ£.B [æ–°å¢] å¤„ç†å¹¶ä¿å­˜ _TIP ç‰ˆæœ¬\n",
    "# ==============================\n",
    "print(\"\\n--- æ­£åœ¨å¤„ç†å¹¶ä¿å­˜ _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬ ---\")\n",
    "\n",
    "# 1. [TIP åˆ’åˆ†] ä½¿ç”¨ \"å¡«è¡¥ç‰ˆ\" çš„ç´¢å¼•æ¥åˆ’åˆ† \"TIPç‰ˆ\" æ•°æ®\n",
    "print(\"    Step 7_TIP: åˆ’åˆ† _TIP æ•°æ® (ä½¿ç”¨ç›¸åŒç´¢å¼•)...\")\n",
    "train_split_tip = train_df_tip.loc[train_indices]\n",
    "val_split_tip = train_df_tip.loc[val_indices]\n",
    "test_split_tip = test_df_tip # Test é›†ä¿æŒå®Œæ•´\n",
    "\n",
    "# 2. [TIP æ ‡å‡†åŒ–] é‡ç”¨ä¹‹å‰ fit å¥½çš„ scaler æ¥ transform\n",
    "#    (StandardScaler ä¼šè‡ªåŠ¨ä¼ æ’­ NaNsï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„)\n",
    "print(\"    Step 8_TIP: æ ‡å‡†åŒ– _TIP æ•°æ® (é‡ç”¨ scaler)...\")\n",
    "if final_continuous_cols: \n",
    "    train_split_tip.loc[:, final_continuous_cols] = scaler.transform(train_split_tip[final_continuous_cols])\n",
    "    val_split_tip.loc[:, final_continuous_cols]   = scaler.transform(val_split_tip[final_continuous_cols])\n",
    "    test_split_tip.loc[:, final_continuous_cols]  = scaler.transform(test_split_tip[final_continuous_cols])\n",
    "\n",
    "# 3. [TIP ä¿å­˜] ä¿å­˜å¸¦ _TIP åç¼€çš„ CSV\n",
    "print(\"    Step 9_TIP: ä¿å­˜ _TIP ç‰¹å¾ CSV...\")\n",
    "for df, name in zip([train_split_tip, val_split_tip, test_split_tip], [\"train\", \"val\", \"test\"]):\n",
    "    if ordered_feature_cols:\n",
    "        features_only_df_tip = df[ordered_feature_cols]\n",
    "        # *** [ä¿®æ”¹] æ·»åŠ  _TIP åç¼€ ***\n",
    "        out_path_tip = os.path.join(OUT_DIR, f\"{name}_features_TIP.csv\")\n",
    "        features_only_df_tip.to_csv(out_path_tip, index=False, header=False) \n",
    "        print(f\"ğŸ’¾ Saved {out_path_tip}\")\n",
    "    else:\n",
    "        print(f\"    (è·³è¿‡ {name}_features_TIP.csvï¼šæœªæ‰¾åˆ°ä»»ä½•ç‰¹å¾)\")\n",
    "print(\"--- _TIP ç‰ˆæœ¬å¤„ç†å®Œæ¯• ---\\n\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ”Ÿ (åŸ Step 9) ä¿å­˜æ ‡ç­¾ .pt æ–‡ä»¶\n",
    "# ==============================\n",
    "print(\"Step 10/11: ä¿å­˜ 4-class æ ‡ç­¾ .pt...\")\n",
    "def save_labels_4c(df, name):\n",
    "    labels = df[new_label_col].astype(int).values\n",
    "    # å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ labels_model_all_{name}_all_views.pt\n",
    "    out_path = os.path.join(OUT_DIR, f\"{name}_labels.pt\")\n",
    "    torch.save(torch.tensor(labels, dtype=torch.long), out_path)\n",
    "    print(f\"ğŸ’¾ Saved {out_path} (Classes: {np.unique(labels)})\")\n",
    "\n",
    "save_labels_4c(train_split, \"train\")\n",
    "save_labels_4c(val_split, \"val\")\n",
    "save_labels_4c(test_split, \"test\")\n",
    "\n",
    "# ==============================\n",
    "# 1ï¸âƒ£1ï¸âƒ£ (å·²ä¿®æ­£) è½¬æ¢å›¾åƒä¸º .NPY (åŒ…å« Resize å’Œå½’ä¸€åŒ–)\n",
    "# ==============================\n",
    "print(\"Step 11/11: è½¬æ¢å›¾åƒä¸º 224x224 .npy å¹¶è¿›è¡Œå½’ä¸€åŒ–...\")\n",
    "\n",
    "def convert_and_save_paths(df, name):\n",
    "    original_jpg_paths = df[image_path_col_final].tolist()\n",
    "    npy_paths = [] \n",
    "    \n",
    "    # ImageNet æ ‡å‡†å½’ä¸€åŒ–å‚æ•°\n",
    "    # --- ä¿®æ”¹å ---\n",
    "    MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 1, 3)\n",
    "    STD = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "    print(f\"\\næ­£åœ¨å¤„ç† {name} å›¾åƒ: Resize(224x224) + RGBè½¬æ¢ + å½’ä¸€åŒ–...\")\n",
    "    for img_path in tqdm(original_jpg_paths, desc=f\"Processing {name}\"):\n",
    "        if img_path is None or pd.isna(img_path):\n",
    "            continue\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        save_path = os.path.splitext(img_path)[0] + \"_processed.npy\"\n",
    "        \n",
    "        try:\n",
    "            # 1. æ‰“å¼€å¹¶å¼ºåˆ¶è½¬ä¸º RGB (å³ä½¿æ˜¯ç°åº¦å›¾ä¹Ÿä¼šå¤åˆ¶æˆ3é€šé“)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # 2. Resize åˆ° ViT è¦æ±‚çš„ 224x224\n",
    "            # ä½¿ç”¨ LANCZOS æ»¤é•œä¿è¯åŒ»ç–—å›¾åƒç¼©æ”¾è´¨é‡\n",
    "            img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # 3. è½¬ä¸º Numpy å¹¶å½’ä¸€åŒ–åˆ° [0, 1]\n",
    "            img_np = np.array(img).astype(np.float32) / 255.0\n",
    "            \n",
    "            # 4. åº”ç”¨ ImageNet å½’ä¸€åŒ– (å‡å‡å€¼ï¼Œé™¤æ ‡å‡†å·®)\n",
    "            img_np = (img_np - MEAN) / STD\n",
    "            \n",
    "            # 5. ç»´åº¦è½¬ç½®: (H, W, C) -> (C, H, W) \n",
    "            # è¿™æ · PyTorch è¯»å–åç›´æ¥å°±èƒ½ç”¨ï¼Œä¸éœ€è¦å† permute\n",
    "            # img_np = img_np.transpose(2, 0, 1)\n",
    "            \n",
    "            # 6. ä¿å­˜\n",
    "            np.save(save_path, img_np)\n",
    "            npy_paths.append(save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    out_path = os.path.join(OUT_DIR, f\"{name}_paths.pt\")\n",
    "    torch.save(npy_paths, out_path)\n",
    "    print(f\"ğŸ’¾ Saved {out_path} (åŒ…å« {len(npy_paths)} æ¡å¤„ç†åçš„ .npy è·¯å¾„)\")\n",
    "\n",
    "convert_and_save_paths(train_split, \"train\")\n",
    "convert_and_save_paths(val_split, \"val\")\n",
    "convert_and_save_paths(test_split, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
