{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0789496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‰§è¡Œ PetFinder æ•°æ®é›†é¢„å¤„ç†æµç¨‹...\n",
      "æ­£åœ¨åŠ è½½æ•°æ® /mnt/hdd/jiazy/petfinder-pawpularity-score/train.csv...\n",
      "æˆåŠŸåŠ è½½ 9912 æ¡è®°å½•ã€‚\n",
      "æ­£åœ¨æ„é€ å›¾åƒè·¯å¾„...\n",
      "æ­£åœ¨å®šä¹‰ç‰¹å¾...\n",
      "æ­£åœ¨æ¸…æ´—æ•°æ®...\n",
      "æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾ç¼–ç ...\n",
      "æ­£åœ¨è¿›è¡Œ 80:10:10 æ•°æ®é›†åˆ’åˆ†...\n",
      "åˆ’åˆ†ç»“æœ: è®­ç»ƒé›† 7929, éªŒè¯é›† 991, æµ‹è¯•é›† 992\n",
      "æ­£åœ¨å¤„ç†å›¾åƒ (Resize & Save .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7929/7929 [04:07<00:00, 31.98it/s]\n",
      "Val Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 991/991 [00:30<00:00, 32.99it/s]\n",
      "Test Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [00:31<00:00, 31.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final] æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ° ./features ...\n",
      "é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure PIL version compatibility (for resize)\n",
    "try:\n",
    "    # PIL 9.0.0+\n",
    "    LANCZOS_RESAMPLE = Image.Resampling.LANCZOS\n",
    "except AttributeError:\n",
    "    # Older PIL\n",
    "    LANCZOS_RESAMPLE = Image.LANCZOS\n",
    "\n",
    "# --- 1. Define constants and paths ---\n",
    "# Please modify the root path here according to your environment\n",
    "DATA_ROOT = \"/mnt/hdd/jiazy/petfinder-pawpularity-score\"\n",
    "METADATA_FILE = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "IMAGE_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./features\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# --- 2. Define feature columns ---\n",
    "# Continuous features\n",
    "CONTINUOUS_COLS = []\n",
    "\n",
    "# Categorical features\n",
    "CATEGORICAL_COLS = [\n",
    "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage'\n",
    "]\n",
    "\n",
    "# Label column\n",
    "LABEL_COL = 'Pawpularity'\n",
    "\n",
    "# Non-feature columns (will be removed)\n",
    "NON_FEATURE_COLS = [\n",
    "    'Id', \n",
    "]\n",
    "\n",
    "# --- 3. Image processing helper function ---\n",
    "def process_and_save_image(png_path):\n",
    "    \"\"\"\n",
    "    Load PNG image, resize to 224x224, and save as .npy file.\n",
    "    If .npy file already exists, skip.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(png_path):\n",
    "        print(f\"Warning: Image file does not exist {png_path}\")\n",
    "        return None  # Return None for later filtering\n",
    "\n",
    "    # Replace .png with .npy\n",
    "    npy_path = png_path.replace(\".jpg\", \".npy\")\n",
    "\n",
    "    if os.path.exists(npy_path):\n",
    "        return npy_path  # File already exists\n",
    "\n",
    "    try:\n",
    "        # Open image\n",
    "        img = Image.open(png_path)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        img_resized = img.resize((224, 224), resample=LANCZOS_RESAMPLE)\n",
    "        \n",
    "        # Convert to NumPy array\n",
    "        if img_resized.mode == 'RGBA':\n",
    "            img_resized = img_resized.convert('RGB')\n",
    "            \n",
    "        np_img = np.array(img_resized)\n",
    "        \n",
    "        # Save as .npy\n",
    "        np.save(npy_path, np_img)\n",
    "        \n",
    "        return npy_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image {png_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_petfinder_flow(metadata_file, image_dir, output_dir, label_col):\n",
    "    \"\"\"\n",
    "    Preprocess PetFinder dataset\n",
    "    \"\"\"\n",
    "    print(\"Starting PetFinder dataset preprocessing pipeline...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. ğŸ“– Load data ---\n",
    "    print(f\"Loading data {metadata_file}...\")\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    print(f\"Successfully loaded {len(df)} records.\")\n",
    "\n",
    "    print(\"Constructing image paths...\")\n",
    "    df['image_path'] = df['Id'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpg\"))\n",
    "\n",
    "\n",
    "    # --- 2. ğŸ“ Define features ---\n",
    "    print(\"Defining features...\")\n",
    "    continuous_cols = CONTINUOUS_COLS\n",
    "    categorical_cols = CATEGORICAL_COLS\n",
    "    non_feature_cols = NON_FEATURE_COLS\n",
    "    exclude_cols = [label_col, 'image_path', 'npy_path'] + non_feature_cols\n",
    "    \n",
    "    # --- 3. ğŸ§¹ Data cleaning ---\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    # Filter rows with missing image paths and labels\n",
    "    df = df.dropna(subset=[label_col])\n",
    "    print(\"Encoding categorical features...\")\n",
    "    cat_dims = [] # Record the number of categories for each feature\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(-1) # Simple filling\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[col] = df[col].cat.codes\n",
    "        # Record category count (max ID + 1)\n",
    "        cat_dims.append(int(df[col].max() + 1))\n",
    "\n",
    "    # --- 6. Dataset splitting ---\n",
    "    print(\"Performing 80:10:10 dataset splitting...\")\n",
    "    # Regression tasks usually don't need stratify, or need bucketed stratify. For simplicity, use random splitting here.\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Split results: Train {len(train_df)}, Validation {len(val_df)}, Test {len(test_df)}\")\n",
    "\n",
    "    # --- 7. Continuous feature standardization (if any) ---\n",
    "    if continuous_cols:\n",
    "        print(\"Standardizing continuous features...\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[continuous_cols])\n",
    "        train_df[continuous_cols] = scaler.transform(train_df[continuous_cols])\n",
    "        val_df[continuous_cols] = scaler.transform(val_df[continuous_cols])\n",
    "        test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])\n",
    "\n",
    "    # --- 8. Image processing ---\n",
    "    print(\"Processing images (Resize & Save .npy)...\")\n",
    "    tqdm.pandas(desc=\"Train Images\")\n",
    "    train_df['npy_path'] = train_df['image_path'].progress_apply(process_and_save_image)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Val Images\")\n",
    "    val_df['npy_path'] = val_df['image_path'].progress_apply(process_and_save_image)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Test Images\")\n",
    "    test_df['npy_path'] = test_df['image_path'].progress_apply(process_and_save_image)\n",
    "\n",
    "    # Filter out failed image processing (those returning None)\n",
    "    len_before = len(train_df) + len(val_df) + len(test_df)\n",
    "    train_df = train_df.dropna(subset=['npy_path'])\n",
    "    val_df = val_df.dropna(subset=['npy_path'])\n",
    "    test_df = test_df.dropna(subset=['npy_path'])\n",
    "    len_after = len(train_df) + len(val_df) + len(test_df)\n",
    "    if len_before != len_after:\n",
    "        print(f\"Warning: Filtered {len_before - len_after} rows with missing or corrupted images.\")\n",
    "\n",
    "    # --- 9. Save output ---\n",
    "    print(f\"[Final] Saving processed files to {output_dir} ...\")\n",
    "\n",
    "    # Save field lengths: categorical first, then continuous (meets TIP model requirements)\n",
    "    tabular_lengths = cat_dims + [1] * len(continuous_cols)\n",
    "    torch.save(tabular_lengths, os.path.join(output_dir, \"tabular_lengths.pt\"))\n",
    "    \n",
    "    for split_name, df_split in zip([\"train\", \"val\", \"test\"], [train_df, val_df, test_df]):\n",
    "        features_path = os.path.join(output_dir, f\"{split_name}_features.csv\")\n",
    "        # Ensure column order: categorical first, then continuous\n",
    "        cols_to_save = categorical_cols + continuous_cols\n",
    "        df_split[cols_to_save].to_csv(features_path, index=False, header=False)\n",
    "        \n",
    "        labels_path = os.path.join(output_dir, f\"{split_name}_labels.pt\")\n",
    "        # [Fixed] Convert labels to float32 for regression\n",
    "        labels_tensor = torch.tensor(df_split[label_col].values, dtype=torch.float32)\n",
    "        torch.save(labels_tensor, labels_path)\n",
    "        \n",
    "        paths_path = os.path.join(output_dir, f\"{split_name}_paths.pt\")\n",
    "        # Save absolute paths of .npy files\n",
    "        npy_path_list = df_split['npy_path'].tolist()\n",
    "        torch.save(npy_path_list, paths_path)\n",
    "\n",
    "    print(\"Preprocessing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_petfinder_flow(\n",
    "        metadata_file=METADATA_FILE,\n",
    "        image_dir=IMAGE_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        label_col=LABEL_COL\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
