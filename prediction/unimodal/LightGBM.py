import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold
# --- ä¿®æ”¹ç‚¹ 1: å¼•å…¥ mean_absolute_error å’Œ r2_score ---
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score
import sys
import torch
import os
from hydra import initialize, compose
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf

def load_and_preprocess_data(cfg: DictConfig):
    task = cfg.get('task', 'classification')
    print(f"--- 1. æ­£åœ¨ä¸º Target: '{cfg.target}' (ä»»åŠ¡: {task}) (é€šç”¨LGBMåŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        X_train = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        X_test = pd.read_csv(cfg.data_test_eval_tabular, header=None)
        
        # åŠ è½½æ ‡ç­¾
        # y_train = torch.load(cfg.labels_train_eval_tabular).numpy()
        # y_test = torch.load(cfg.labels_test_eval_tabular).numpy()
        y_train = torch.load(cfg.labels_train_eval_tabular, map_location="cpu")

        if isinstance(y_train, torch.Tensor):
            y_train = y_train.detach().cpu().numpy()
        else:
            y_train = np.asarray(y_train)

        y_train = y_train.reshape(-1)  # (N,1) -> (N,)

        y_test = torch.load(cfg.labels_test_eval_tabular, map_location="cpu")

        if isinstance(y_test, torch.Tensor):
            y_test = y_test.detach().cpu().numpy()
        else:
            y_test = np.asarray(y_test)

        y_test = y_test.reshape(-1)



        print("    æ•°æ®åŠ è½½æˆåŠŸã€‚")

        # --- 2. åŠ è½½å­—æ®µé•¿åº¦ (ç”¨äºè‡ªåŠ¨è¯†åˆ«ç±»åˆ«ç‰¹å¾) ---
        # [!] å…³é”®ä¿®æ”¹ï¼šè¯»å– field_lengths
        all_field_lengths = torch.load(cfg.field_lengths_tabular)
        if isinstance(all_field_lengths, torch.Tensor):
            all_field_lengths = all_field_lengths.tolist()

        # ç®€å•çš„æ ¡éªŒ
        if X_train.shape[1] != len(all_field_lengths):
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({X_train.shape[1]}) ä¸ field_lengths é•¿åº¦ ({len(all_field_lengths)}) ä¸ä¸€è‡´ï¼")
            sys.exit(1)

        # --- 3. æ ‡ç­¾å¤„ç† (1-indexed -> 0-indexed) ---
        if task == 'classification':
            label_min = np.min(y_train)
            label_max = np.max(y_train)
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ï¼Œæ­£åœ¨ä¿®æ­£...")
                y_train = y_train - 1
                y_test = y_test - 1

        # --- 4. è½¬æ¢åˆ†ç±»ç‰¹å¾ (æ ¸å¿ƒä¿®æ”¹) ---
        
        # è‡ªåŠ¨è¯†åˆ«ï¼šé•¿åº¦ > 1 çš„æ˜¯ç±»åˆ«ç‰¹å¾
        cat_indices = [i for i, length in enumerate(all_field_lengths) if length > 1]
        
        print(f"    è‡ªåŠ¨æ£€æµ‹åˆ° {len(cat_indices)} ä¸ªç±»åˆ«ç‰¹å¾ (æ ¹æ® field_lengths > 1)ã€‚")

        if len(cat_indices) > 0:
            # ä¸ºäº†é¿å… pandas çš„ SettingWithCopyWarning æˆ–ç±»å‹æ··æ·†ï¼Œ
            # å»ºè®®ç»™åˆ—é‡å‘½åä¸ºå­—ç¬¦ä¸²ï¼Œè¿™æ ·å¤„ç†èµ·æ¥æ›´æ¸…æ™°
            X_train.columns = [str(i) for i in range(X_train.shape[1])]
            X_test.columns  = [str(i) for i in range(X_test.shape[1])]

            # ä»…å°†æ£€æµ‹åˆ°çš„ç±»åˆ«åˆ—è½¬æ¢ä¸º 'category' ç±»å‹
            for idx in cat_indices:
                col_name = str(idx)
                # è½¬æ¢ä¸º category
                X_train[col_name] = X_train[col_name].astype('category')
                
                # å¯¹é½æµ‹è¯•é›† (å¤„ç†æœªçŸ¥ç±»åˆ«)
                # set_categories ç¡®ä¿æµ‹è¯•é›†å³ä½¿æœ‰æœªè§è¿‡çš„ç±»åˆ«ä¹Ÿä¸ä¼šæŠ¥é”™(ä¼šå˜æˆNaN)ï¼Œ
                # æˆ–è€…ç¡®ä¿å…¶ç±»åˆ«åˆ—è¡¨ä¸è®­ç»ƒé›†ä¸€è‡´
                X_test[col_name] = pd.Categorical(X_test[col_name], categories=X_train[col_name].cat.categories, ordered=False)
            
            print("    å·²å°†ç±»åˆ«ç‰¹å¾è½¬æ¢ä¸º pandas 'category' dtypeã€‚LightGBM å°†è‡ªåŠ¨è¯†åˆ«å®ƒä»¬ã€‚")
        else:
            print("    æœªæ£€æµ‹åˆ°ç±»åˆ«ç‰¹å¾ï¼Œæ‰€æœ‰åˆ—å°†ä½œä¸ºæ•°å€¼å¤„ç†ã€‚")

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # --- 5. ç¡®å®šé—®é¢˜ç±»å‹ (ä¿æŒä¸å˜) ---
    print("-" * 30)
    if task == 'classification':
        num_classes = cfg.get('num_classes', len(np.unique(y_train))) 
        if num_classes == 2:
            problem_type = 'binary'; objective = 'binary'; num_class_param = {}; scoring_metric = 'roc_auc'
        else:
            problem_type = 'multiclass'; objective = 'multiclass'; num_class_param = {'num_class': num_classes}; scoring_metric = 'accuracy'
    elif task == 'regression':
        problem_type = 'regression'; objective = 'regression_l2'; num_class_param = {}; scoring_metric = 'neg_root_mean_squared_error'
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹ '{task}'"); sys.exit(1)

    print(f"LGBM Objective: {objective}, Scoring: {scoring_metric}")
    
    return X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric


def get_model_and_grid(problem_type, objective, num_class_param, seed):
    """
    æ ¹æ®é—®é¢˜ç±»å‹è·å–LGBMæ¨¡å‹å’Œå‚æ•°ç½‘æ ¼ã€‚
    """
    if problem_type in ['binary', 'multiclass']:
        model = lgb.LGBMClassifier(
            objective=objective,
            **num_class_param,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    elif problem_type == 'regression':
        model = lgb.LGBMRegressor(
            objective=objective,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    
    # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šä¿®æ”¹ param_grid â†“â†“â†“ ---
    param_grid = {
        'num_leaves': [31, 127],
        'learning_rate': [0.01, 0.1],
        'min_child_samples': [20, 50, 100],
        'min_sum_hessian_in_leaf': [1e-3, 1e-2, 1e-1],
        
        # --- å°†é‡‡æ ·å‚æ•°æ·»åŠ åˆ°ç½‘æ ¼æœç´¢ä¸­ ---
        'feature_fraction': [0.8, 0.9], # æœç´¢ 80% æˆ– 90% çš„ç‰¹å¾
        'bagging_fraction': [0.8, 0.9]  # æœç´¢ 80% æˆ– 90% çš„æ•°æ®
    }
    
    return model, param_grid


def run_experiment(X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric, seed):
    """
    ä½¿ç”¨ç»™å®šçš„éšæœºç§å­è¿è¡Œä¸€æ¬¡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚
    """
    print(f"\n{'='*25} ---------------- éšæœºç§å­: {seed} ---------------- {'='*25}")
    
    model, param_grid = get_model_and_grid(problem_type, objective, num_class_param, seed)

    print(f"å¼€å§‹è¿›è¡Œç½‘æ ¼æœç´¢ (è¯„åˆ†æŒ‡æ ‡: {scoring_metric})...")
    
    if problem_type == 'regression':
        cv_splitter = KFold(n_splits=5, shuffle=True, random_state=seed)
    else:
        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring_metric,
        cv=cv_splitter,
        n_jobs=-1, # GridSearchCV ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
        verbose=1
    )
    grid_search.fit(X_train, y_train)

    print("ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°: {grid_search.best_params_}")
    print(f"åœ¨äº¤å‰éªŒè¯ä¸­çš„æœ€ä½³ {scoring_metric}: {grid_search.best_score_:.4f}")
    print("-" * 30)

    print("ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†(éªŒè¯é›†)ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    
    if problem_type in ['binary', 'multiclass']:
        y_pred_proba = best_model.predict_proba(X_test)
        acc = accuracy_score(y_test, y_pred)
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        
        if problem_type == 'binary':
            auc = roc_auc_score(y_test, y_pred_proba[:, 1])
        else:
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
        
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
    
    elif problem_type == 'regression':
        # --- ä¿®æ”¹ç‚¹ 2: å¢åŠ  MAE å’Œ R2 çš„è®¡ç®— ---
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        result_line = f"rmse:{rmse:.4f},mae:{mae:.4f},r2:{r2:.4f}"

    print("è¯„ä¼°ç»“æœ:")
    print(result_line)
    
    return result_line, grid_search.best_params_

def call_lgb_with_config(config_name: str):
    """
    Functional interface to run the LightGBM experiment with a specific config.
    """
    # 1. Initialize and Compose the configuration
    # Ensure config_path points correctly to your yaml directory
    with initialize(version_base=None, config_path="../configs"):
        # Load the specific config file
        cfg = compose(config_name=config_name)
    
    # 2. Run the core experiment logic
    return run_lgb_experiment(cfg)

def run_lgb_experiment(cfg: DictConfig):
    """
    Core logic for LightGBM path resolution and experiment execution.
    """
    
    # --- 1.A. Parse Data Paths ---
    print("--- 1.A. Parsing Data Paths ---")
    data_root = cfg.get('data_base') 
    
    if data_root is not None:
        print(f"    Detected 'data_root', prepending to all path keys: {data_root}")
        
        # Define all path keys that require prefixing
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        
        # Traverse keys and update paths if they exist
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                cfg[key] = os.path.join(data_root, original_path)
    else:
        print("    No 'data_root' provided. Assuming paths are already absolute or relative to CWD.")

    print("\n--- Final Configuration (Paths Resolved): ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Current Working Directory: {os.getcwd()}")
    print("--------------------")

    # 1. Load and preprocess data
    X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric = load_and_preprocess_data(cfg)

    # --- Handle Dataset Name ---
    # When using Compose API, HydraConfig might not be populated automatically
    try:
        dataset_name = HydraConfig.get().runtime.choices.get("dataset", "unknown_dataset")
    except Exception:
        # Fallback: try to get dataset name from config or use 'manual_run'
        dataset_name = cfg.get('target', "manual_run")

    output_filename = os.path.join("result", f"lgb_results_{dataset_name}.txt")
    os.makedirs(os.path.dirname(output_filename), exist_ok=True)
    seeds = [2022, 2023, 2024]
    
    # 3. Open file to write results
    print(f"\nPreparing to write results to: {output_filename}")
    
    with open(output_filename, 'a') as f:
        f.write("--- Final Config ---\n")
        f.write(OmegaConf.to_yaml(cfg))
        f.write("-" * 30 + "\n\n")

        # 4. Iterate through random seeds
        for seed in seeds:
            result_line, best_params = run_experiment(
                X_train, y_train, X_test, y_test,
                problem_type, objective, num_class_param, scoring_metric,
                seed
            )
            
            # 5. Log results
            print(f"Writing results for seed {seed} to {output_filename}...")
            f.write(f"seed:{seed}\n")
            f.write(f"best_params: {best_params}\n")
            f.write(result_line + "\n\n")

    print(f"\nTask Complete! Results saved at: '{os.path.join(os.getcwd(), output_filename)}'")
    return output_filename

