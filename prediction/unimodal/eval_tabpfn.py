# eval_tabpfn.py
# -*- coding: utf-8 -*-
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import os
import sys
import torch
import random

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
# [ä¿®æ”¹] å¼•å…¥å›å½’æŒ‡æ ‡
from sklearn.metrics import (
    accuracy_score, f1_score, log_loss, roc_auc_score,
    mean_squared_error, mean_absolute_error, r2_score
)

# [ä¿®æ”¹] å¼•å…¥ Regressor
from tabpfn import TabPFNClassifier, TabPFNRegressor
from tabpfn_extensions.many_class.many_class_classifier import ManyClassClassifier

from hydra import initialize, compose
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf

# =========================
# å·¥å…·å‡½æ•°
# =========================

TEXT_LENGTH_DROP_THRESHOLD = 30
HIGH_CARDINALITY_THRESHOLD = 200
N_ENSEMBLE_CONFIGURATIONS = 16

def load_data(cfg: DictConfig):
    """
    (é‡æ„ç‰ˆ - åŸºäº field_lengths è‡ªåŠ¨åˆ¤æ–­åˆ—ç±»å‹)
    """
    import sys
    import numpy as np
    import pandas as pd
    import torch

    target = cfg.target
    print(f"[INFO] æ­£åœ¨åŠ è½½ target: {target} (è‡ªåŠ¨æ¨æ–­åˆ—ç±»å‹)")

    def to_numpy(x):
        """æŠŠ torch.Tensor / numpy.ndarray / list ç­‰ç»Ÿä¸€è½¬æˆ numpy.ndarray"""
        if isinstance(x, torch.Tensor):
            return x.detach().cpu().numpy()
        if isinstance(x, np.ndarray):
            return x
        return np.asarray(x)

    def postprocess_y(y_np, task: str):
        """æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿä¸€ y çš„ dtype/shape"""
        y_np = to_numpy(y_np)

        if task == "regression":
            # å›å½’ï¼šfloat32ï¼Œä¿ç•™åŸ shapeï¼ˆå¸¸è§ä¸º (N,) æˆ– (N,1)ï¼‰
            return y_np.astype(np.float32)

        # åˆ†ç±»ï¼šå¸Œæœ›æ˜¯ (N,) çš„ int64
        y_np = y_np
        if y_np.ndim > 1 and y_np.shape[-1] == 1:
            y_np = y_np.reshape(-1)

        # å¦‚æœæ ‡ç­¾è¢«å­˜æˆ floatï¼ˆ0.0/1.0/2.0ï¼‰ï¼Œè½¬æˆ int æ›´ç¨³
        if np.issubdtype(y_np.dtype, np.floating):
            y_np = y_np.astype(np.int64)
        else:
            y_np = y_np.astype(np.int64, copy=False)

        return y_np

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        X_train_full = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        y_train_obj = torch.load(cfg.labels_train_eval_tabular, weights_only=False)
        y_train_full = postprocess_y(y_train_obj, cfg.task)

        X_test_full = pd.read_csv(cfg.data_test_eval_tabular, header=None)
        y_test_obj = torch.load(cfg.labels_test_eval_tabular, weights_only=False)
        y_test_full = postprocess_y(y_test_obj, cfg.task)

        # --- 2. åŠ è½½ field_lengths å¹¶è®¡ç®—ç´¢å¼• ---
        field_lengths_path = cfg.field_lengths_tabular
        print(f"[INFO] è¯»å–å­—æ®µé•¿åº¦æ–‡ä»¶: {field_lengths_path}")

        try:
            field_lengths_obj = torch.load(field_lengths_path, weights_only=False)
            field_lengths = to_numpy(field_lengths_obj)
        except Exception:
            field_lengths = np.load(field_lengths_path)

        field_lengths = np.array(field_lengths).flatten()

        n_cols_data = X_train_full.shape[1]
        n_cols_lengths = len(field_lengths)
        if n_cols_data != n_cols_lengths:
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({n_cols_data}) ä¸ field_lengths é•¿åº¦ ({n_cols_lengths}) ä¸åŒ¹é…ï¼")
            sys.exit(1)

        con_indices = [i for i, fl in enumerate(field_lengths) if fl == 1]
        cat_indices = [i for i, fl in enumerate(field_lengths) if fl > 1]

        print(f"[INFO] è‡ªåŠ¨æ£€æµ‹ç»“æœ:")
        print(f"      - æ•°å€¼åˆ—æ•°é‡: {len(con_indices)}")
        print(f"      - ç±»åˆ«åˆ—æ•°é‡: {len(cat_indices)}")

        # --- 3. å®šä¹‰åˆ—å ---
        all_col_names = [f"col_{i}" for i in range(n_cols_data)]
        X_train_full.columns = all_col_names
        X_test_full.columns = all_col_names

        num_cols = [all_col_names[i] for i in con_indices]
        cat_cols = [all_col_names[i] for i in cat_indices]

        # --- 4. æ ‡ç­¾å¤„ç† (1-indexed -> 0-indexed) ---
        # åªæœ‰åˆ†ç±»ä»»åŠ¡æ‰æ‰§è¡Œ
        if cfg.task == "classification":
            label_min = int(np.min(y_train_full)) if y_train_full.size > 0 else 0
            label_max = int(np.max(y_train_full)) if y_train_full.size > 0 else 0
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ï¼Œæ­£åœ¨ä¿®å¤...")
                y_train_full = y_train_full - 1
                y_test_full = y_test_full - 1

        # --- 5. å¼ºåˆ¶ç±»å‹è½¬æ¢ ---
        if cat_cols:
            for col in cat_cols:
                X_train_full[col] = X_train_full[col].astype(str)
                X_test_full[col] = X_test_full[col].astype(str)

        if num_cols:
            for col in num_cols:
                X_train_full[col] = pd.to_numeric(X_train_full[col], errors="coerce").fillna(0)
                X_test_full[col] = pd.to_numeric(X_test_full[col], errors="coerce").fillna(0)

        # ï¼ˆå¯é€‰ï¼‰è°ƒè¯•è¾“å‡ºï¼Œç¡®è®¤ç±»å‹å’Œå½¢çŠ¶ï¼Œç¡®è®¤æ²¡é—®é¢˜åå¯åˆ 
        print(f"[DEBUG] X_train: {X_train_full.shape}, y_train: {y_train_full.shape}, {y_train_full.dtype}")
        print(f"[DEBUG] X_test : {X_test_full.shape}, y_test : {y_test_full.shape}, {y_test_full.dtype}")
        print(f"[DEBUG] num_cols={len(num_cols)}, cat_cols={len(cat_cols)}")

        return X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def build_preprocess(num_cols, cat_cols):
    transformers = []
    if num_cols:
        transformers.append(("num", StandardScaler(), num_cols))
    if cat_cols:
        transformers.append(("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols))
    
    return ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)

def get_subsample_indices(y, sample_size, seed, task):
    """
    [ä¿®æ”¹] é€šç”¨é‡‡æ ·å‡½æ•°ï¼š
    - åˆ†ç±»ä»»åŠ¡ï¼šåˆ†å±‚é‡‡æ ·
    - å›å½’ä»»åŠ¡ï¼šéšæœºé‡‡æ ·
    """
    sample_size = int(sample_size)
    if len(y) <= sample_size:
        return np.arange(len(y))
    
    # 1. å›å½’ä»»åŠ¡ç›´æ¥éšæœºé‡‡æ ·
    if task == 'regression':
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    # 2. åˆ†ç±»ä»»åŠ¡é€»è¾‘
    unique_classes, counts = np.unique(y, return_counts=True)
    if len(unique_classes) < 2 or (counts < 2).any():
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    sss = StratifiedShuffleSplit(n_splits=1, train_size=sample_size, random_state=seed)
    idx_all = np.arange(len(y))
    try:
        for sub_idx, _ in sss.split(idx_all, y):
            return sub_idx
    except ValueError:
        np.random.seed(seed)
        return np.random.choice(idx_all, sample_size, replace=False)

def evaluate_metrics(y_true, y_pred, task, y_proba=None):
    """
    [ä¿®æ”¹] æ”¯æŒå›å½’å’Œåˆ†ç±»æŒ‡æ ‡
    """
    res = {}
    
    if task == 'classification':
        res["accuracy"] = accuracy_score(y_true, y_pred)
        res["macro_f1"] = f1_score(y_true, y_pred, average='macro')
        res["weighted_f1"] = f1_score(y_true, y_pred, average='weighted')
        
        if y_proba is not None:
            try:
                if y_proba.shape[1] == 2:
                    res["auc"] = roc_auc_score(y_true, y_proba[:, 1])
                else:
                    res["auc_macro_ovr"] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
            except:
                pass
                
    elif task == 'regression':
        # [æ–°å¢] å›å½’æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        res["rmse"] = rmse
        res["mae"] = mae
        res["r2"] = r2
        
    return res

# =========================
# ä¸»æµç¨‹
# =========================

def call_with_specific_config(config_name: str):
    """
    Functional interface to run the experiment with a specific config file.
    """
    # 1. Initialize Hydra and compose the configuration
    # config_path is relative to this python file
    with initialize(version_base=None, config_path="../configs"):
        # We load the specific config_name passed as an argument
        cfg = compose(config_name=config_name)
        
    # 2. Call the core logic (original main function logic)
    return run_tabpfn_experiment(cfg)

def run_tabpfn_experiment(cfg: DictConfig):
    """
    Core logic extracted from the original main function.
    """
    seeds = [2022, 2023, 2024]
    results_all = []

    # Ensure 'task' field exists in cfg
    if 'task' not in cfg:
        print("âš ï¸ Missing 'task' in config, defaulting to 'classification'")
        cfg.task = 'classification'

    for seed in seeds:
        print(f"\nğŸš€ Running seed = {seed} | Task: {cfg.task}")
        cfg.seed = seed

        # --- Path Resolution ---
        data_root = cfg.get('data_base')
        if data_root:
            path_keys = [
                'labels_train_eval_tabular', 'labels_test_eval_tabular',
                'data_train_eval_tabular', 'data_test_eval_tabular',
                'field_lengths_tabular'
            ]
            for key in path_keys:
                if key in cfg and cfg[key] and not os.path.isabs(cfg[key]):
                    cfg[key] = os.path.join(data_root, cfg[key])

        TRAIN_SAMPLE_THRESHOLD = cfg.get('train_sample_max', 10000)
        TEST_SAMPLE_THRESHOLD = cfg.get('test_sample_max', 10000)

        # 1. Load Data
        X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols = load_data(cfg)

        # 2. Preprocessing
        preprocess = build_preprocess(num_cols, cat_cols)

        # 3. Subsampling
        sample_size = min(len(y_train_full), TRAIN_SAMPLE_THRESHOLD)
        sub_idx = get_subsample_indices(y_train_full, sample_size, seed, cfg.task)
        X_train_sampled = X_train_full.iloc[sub_idx]
        y_train_sampled = y_train_full[sub_idx]

        # 4. Feature Transformation
        print("Preprocessing features...")
        X_train_np = preprocess.fit_transform(X_train_sampled)
        X_test_np  = preprocess.transform(X_test_full)

        # 5. Model Initialization
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if cfg.task == 'classification':
            if cfg.num_classes > 10:
                base_clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
                clf = ManyClassClassifier(estimator=base_clf, alphabet_size=10, random_state=seed)
            else:
                clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
        elif cfg.task == 'regression':
            print("Initializing TabPFNRegressor...")
            clf = TabPFNRegressor(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
        else:
            raise ValueError(f"Unknown task type: {cfg.task}")

        # 6. Training
        clf.fit(X_train_np, y_train_sampled)

        # 7. Evaluation Sampling
        X_test_eval, y_test_eval = X_test_np, y_test_full
        if len(X_test_np) > TEST_SAMPLE_THRESHOLD:
            stratify_target = y_test_full if cfg.task == 'classification' else None
            X_test_eval, _, y_test_eval, _ = train_test_split(
                X_test_np, y_test_full,
                train_size=TEST_SAMPLE_THRESHOLD,
                stratify=stratify_target,
                random_state=seed
            )
        
        # 8. Prediction
        test_proba = None
        if cfg.task == 'classification':
            test_proba = clf.predict_proba(X_test_eval)
            test_pred  = np.argmax(test_proba, axis=1)
        else:
            test_pred = clf.predict(X_test_eval)
        
        # 9. Metrics
        metrics = evaluate_metrics(y_test_eval, test_pred, cfg.task, test_proba)
        results_all.append({"seed": seed, "results": metrics})

    # Save results
    output_file = cfg.get('output_file', "result/tabpfn_results.json")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(results_all, f, indent=2)
    
    print(f"Results saved to: {output_file}")
    
    # Return path or metrics if needed for higher-level logic
    return output_file
